## Playing 2048 Using Deep Reinforcement Learning
<p>After reading a book on AlphaGo, I decided to make an implementation of MCTS + Deep Reinforcement Learning for 2048.
This Project uses a DDQN with a PER to estimate Q(s,a) for a given game state and action.
Q(s,a) is combined with Monte Carlo Tree Search (MCTS) to produce a better estimate of the most optimal move in the current situation.</p>

## Todo
- Add Adaptive search depth for MCTS (Some states have fewer empty squares or more high value tiles, expand the search depth for these states add Expectimax Search
- Improved action selection for selection step in MCTS. (Currently MCTS selects the action randomly when performing rollouts.
Instead of this, we can use the Q(s,a) values generated by the dqn to produce probabilities for each move with better moves being selected more often.)
- Improved parallization code for PER. Currently PER does not support storing data on the GPU, because of the storage format of the data. Store all data in a torch tensor in the future.
- Better data parallization
- convert all data to torch tensors to reduce time wastage switching between cpu and gpu. 
- move all tensors to gpu